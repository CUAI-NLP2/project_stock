{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapping(date):\n",
    "    \n",
    "    page=[]\n",
    "    for i in range(1,1000):\n",
    "        page.append(str(i))\n",
    "        \n",
    "    with open('raw_data.txt','a', encoding='utf-8') as f:\n",
    "        for i in range(len(date)):\n",
    "            f.write(date[i]+'\\n')\n",
    "            # 가장 많이 본 뉴스 마지막 페이지 구하기\n",
    "            url = \"https://finance.naver.com/news/news_list.nhn?mode=RANK&date=\"+date[i]+\"&page=10000\"\n",
    "            html = requests.get(url).text\n",
    "            soup = BeautifulSoup(html,\"html5lib\")\n",
    "            tags = soup.select(\"tbody tr td table tbody tr td\")\n",
    "            tag = tags[-1]\n",
    "            rr = int(tag.text) #마지막 페이지\n",
    "\n",
    "\n",
    "            # 각 페이지마다 크롤링\n",
    "            for k in range(rr):\n",
    "                url = \"https://finance.naver.com/news/news_list.nhn?mode=RANK&date=\"+date[i]+\"&page=\"+page[k+1]\n",
    "                html= urlopen(url)\n",
    "                bs_parser=BeautifulSoup(html, 'html.parser')\n",
    "                \n",
    "                #각 페이지의 뉴스링크에 들어가서 제목 크롤링\n",
    "                for link in bs_parser.find('div',{'class':'hotNewsList'}).findAll('a',href=re.compile('^(/news/)')): #페이지의 뉴스링크들\n",
    "                    if 'href' in link.attrs:\n",
    "                        a= link.attrs['href']\n",
    "                        url = 'https://finance.naver.com'+a\n",
    "                        html= urlopen(url) #뉴스링크에 들어가기\n",
    "                        bs_parser=BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "                        try:\n",
    "                            title=bs_parser.find('div',{'class':'article_info'}).find('h3').get_text().strip() #기사페이지에서 제목 크롤링\n",
    "                        except:\n",
    "                            continue\n",
    "                            \n",
    "                        f.write(title+'\\n')\n",
    "                        \n",
    "            f.write('\\n')\n",
    "            \n",
    "            if i % 10 ==0 :\n",
    "                print('%d 번째, %s 까지 크롤링 완료!' %(i,date[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_update():\n",
    "    dfdf = pd.read_csv(\"preprocessed_data_final.csv\")\n",
    "    date = list(np.array(dfdf[\"날짜\"].tolist()))\n",
    "    for i in range(len(date)):\n",
    "        date[i] = date[i].replace('-','')\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째, 20010108 까지 크롤링 완료!\n",
      "10 번째, 20010122 까지 크롤링 완료!\n",
      "20 번째, 20010208 까지 크롤링 완료!\n",
      "30 번째, 20010222 까지 크롤링 완료!\n",
      "40 번째, 20010309 까지 크롤링 완료!\n",
      "50 번째, 20010323 까지 크롤링 완료!\n",
      "60 번째, 20010409 까지 크롤링 완료!\n",
      "70 번째, 20010423 까지 크롤링 완료!\n",
      "80 번째, 20010508 까지 크롤링 완료!\n",
      "90 번째, 20010522 까지 크롤링 완료!\n",
      "100 번째, 20010605 까지 크롤링 완료!\n",
      "110 번째, 20010620 까지 크롤링 완료!\n",
      "120 번째, 20010704 까지 크롤링 완료!\n",
      "130 번째, 20010719 까지 크롤링 완료!\n",
      "140 번째, 20010802 까지 크롤링 완료!\n",
      "150 번째, 20010817 까지 크롤링 완료!\n",
      "160 번째, 20010831 까지 크롤링 완료!\n",
      "170 번째, 20010914 까지 크롤링 완료!\n",
      "180 번째, 20010928 까지 크롤링 완료!\n",
      "190 번째, 20011017 까지 크롤링 완료!\n",
      "200 번째, 20011031 까지 크롤링 완료!\n",
      "210 번째, 20011114 까지 크롤링 완료!\n",
      "220 번째, 20011128 까지 크롤링 완료!\n",
      "230 번째, 20011212 까지 크롤링 완료!\n",
      "240 번째, 20011227 까지 크롤링 완료!\n",
      "250 번째, 20020114 까지 크롤링 완료!\n",
      "260 번째, 20020128 까지 크롤링 완료!\n",
      "270 번째, 20020214 까지 크롤링 완료!\n",
      "280 번째, 20020228 까지 크롤링 완료!\n",
      "290 번째, 20020315 까지 크롤링 완료!\n",
      "300 번째, 20020329 까지 크롤링 완료!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-c14fd3540d55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscrapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-122-a6627a73d748>\u001b[0m in \u001b[0;36mscrapping\u001b[1;34m(date)\u001b[0m\n\u001b[0;32m     30\u001b[0m                         \u001b[0mhtml\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#뉴스링크에 들어가기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                         \u001b[0mbs_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                         \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbs_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'article_info'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#기사페이지에서 제목 크롤링\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# date = date_update()\n",
    "# scrapping(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapping_test(date):\n",
    "    \n",
    "    page=[]\n",
    "    for i in range(1,1000):\n",
    "        page.append(str(i))\n",
    "        \n",
    "    with open('raw_data_test.txt','a', encoding='utf-8') as f:\n",
    "        for i in range(len(date)):\n",
    "            f.write(date[i]+'\\n')\n",
    "            # 가장 많이 본 뉴스 마지막 페이지 구하기\n",
    "            url = \"https://finance.naver.com/news/news_list.nhn?mode=RANK&date=\"+date[i]+\"&page=10000\"\n",
    "            html = requests.get(url).text\n",
    "            soup = BeautifulSoup(html,\"html5lib\")\n",
    "            tags = soup.select(\"tbody tr td table tbody tr td\")\n",
    "            tag = tags[-1]\n",
    "            rr = int(tag.text) #마지막 페이지\n",
    "\n",
    "\n",
    "            # 각 페이지마다 크롤링\n",
    "            for k in range(rr):\n",
    "                url = \"https://finance.naver.com/news/news_list.nhn?mode=RANK&date=\"+date[i]+\"&page=\"+page[k+1]\n",
    "                html= urlopen(url)\n",
    "                bs_parser=BeautifulSoup(html, 'html.parser')\n",
    "                \n",
    "                #각 페이지의 뉴스링크에 들어가서 제목 크롤링\n",
    "                for link in bs_parser.find('div',{'class':'hotNewsList'}).findAll('a',href=re.compile('^(/news/)')): #페이지의 뉴스링크들\n",
    "                    if 'href' in link.attrs:\n",
    "                        a= link.attrs['href']\n",
    "                        url = 'https://finance.naver.com'+a\n",
    "                        html= urlopen(url) #뉴스링크에 들어가기\n",
    "                        bs_parser=BeautifulSoup(html, 'html.parser')\n",
    "                        try:\n",
    "                            title=bs_parser.find('div',{'class':'article_info'}).find('h3').get_text().strip() #기사페이지에서 제목 크롤링\n",
    "                        except:\n",
    "                            continue\n",
    "                        \n",
    "                        f.write(title+'\\n')\n",
    "                        \n",
    "            f.write('\\n')\n",
    "            \n",
    "            if i % 10 ==0 :\n",
    "                print('%d 번째, %s 까지 크롤링 완료!' %(i,date[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째, 20020403 까지 크롤링 완료!\n",
      "10 번째, 20020418 까지 크롤링 완료!\n",
      "20 번째, 20020503 까지 크롤링 완료!\n",
      "30 번째, 20020517 까지 크롤링 완료!\n",
      "40 번째, 20020531 까지 크롤링 완료!\n",
      "50 번째, 20020618 까지 크롤링 완료!\n",
      "60 번째, 20020703 까지 크롤링 완료!\n",
      "70 번째, 20020718 까지 크롤링 완료!\n",
      "80 번째, 20020801 까지 크롤링 완료!\n",
      "90 번째, 20020816 까지 크롤링 완료!\n",
      "100 번째, 20020830 까지 크롤링 완료!\n",
      "110 번째, 20020913 까지 크롤링 완료!\n",
      "120 번째, 20020930 까지 크롤링 완료!\n",
      "130 번째, 20021015 까지 크롤링 완료!\n",
      "140 번째, 20021029 까지 크롤링 완료!\n",
      "150 번째, 20021112 까지 크롤링 완료!\n",
      "160 번째, 20021126 까지 크롤링 완료!\n",
      "170 번째, 20021210 까지 크롤링 완료!\n",
      "180 번째, 20021226 까지 크롤링 완료!\n",
      "190 번째, 20030113 까지 크롤링 완료!\n",
      "200 번째, 20030127 까지 크롤링 완료!\n",
      "210 번째, 20030211 까지 크롤링 완료!\n",
      "220 번째, 20030225 까지 크롤링 완료!\n",
      "230 번째, 20030311 까지 크롤링 완료!\n",
      "240 번째, 20030325 까지 크롤링 완료!\n",
      "250 번째, 20030408 까지 크롤링 완료!\n",
      "260 번째, 20030422 까지 크롤링 완료!\n",
      "270 번째, 20030509 까지 크롤링 완료!\n",
      "280 번째, 20030523 까지 크롤링 완료!\n",
      "290 번째, 20030609 까지 크롤링 완료!\n",
      "300 번째, 20030623 까지 크롤링 완료!\n",
      "310 번째, 20030707 까지 크롤링 완료!\n",
      "320 번째, 20030722 까지 크롤링 완료!\n",
      "330 번째, 20030805 까지 크롤링 완료!\n",
      "340 번째, 20030820 까지 크롤링 완료!\n",
      "350 번째, 20030903 까지 크롤링 완료!\n",
      "360 번째, 20030922 까지 크롤링 완료!\n",
      "370 번째, 20031007 까지 크롤링 완료!\n",
      "380 번째, 20031021 까지 크롤링 완료!\n",
      "390 번째, 20031104 까지 크롤링 완료!\n",
      "400 번째, 20031118 까지 크롤링 완료!\n",
      "410 번째, 20031202 까지 크롤링 완료!\n",
      "420 번째, 20031216 까지 크롤링 완료!\n",
      "430 번째, 20040102 까지 크롤링 완료!\n",
      "440 번째, 20040116 까지 크롤링 완료!\n",
      "450 번째, 20040204 까지 크롤링 완료!\n",
      "460 번째, 20040218 까지 크롤링 완료!\n",
      "470 번째, 20040304 까지 크롤링 완료!\n",
      "480 번째, 20040318 까지 크롤링 완료!\n",
      "490 번째, 20040401 까지 크롤링 완료!\n",
      "500 번째, 20040419 까지 크롤링 완료!\n",
      "510 번째, 20040503 까지 크롤링 완료!\n",
      "520 번째, 20040518 까지 크롤링 완료!\n",
      "530 번째, 20040602 까지 크롤링 완료!\n",
      "540 번째, 20040616 까지 크롤링 완료!\n",
      "550 번째, 20040630 까지 크롤링 완료!\n",
      "560 번째, 20040714 까지 크롤링 완료!\n",
      "570 번째, 20040728 까지 크롤링 완료!\n",
      "580 번째, 20040811 까지 크롤링 완료!\n",
      "590 번째, 20040825 까지 크롤링 완료!\n",
      "600 번째, 20040908 까지 크롤링 완료!\n",
      "610 번째, 20040922 까지 크롤링 완료!\n",
      "620 번째, 20041011 까지 크롤링 완료!\n",
      "630 번째, 20041025 까지 크롤링 완료!\n",
      "640 번째, 20041108 까지 크롤링 완료!\n",
      "650 번째, 20041122 까지 크롤링 완료!\n",
      "660 번째, 20041206 까지 크롤링 완료!\n",
      "670 번째, 20041220 까지 크롤링 완료!\n",
      "680 번째, 20050104 까지 크롤링 완료!\n",
      "690 번째, 20050118 까지 크롤링 완료!\n",
      "700 번째, 20050201 까지 크롤링 완료!\n",
      "710 번째, 20050218 까지 크롤링 완료!\n",
      "720 번째, 20050307 까지 크롤링 완료!\n",
      "730 번째, 20050321 까지 크롤링 완료!\n",
      "740 번째, 20050404 까지 크롤링 완료!\n",
      "750 번째, 20050419 까지 크롤링 완료!\n",
      "760 번째, 20050503 까지 크롤링 완료!\n",
      "770 번째, 20050518 까지 크롤링 완료!\n",
      "780 번째, 20050601 까지 크롤링 완료!\n",
      "790 번째, 20050616 까지 크롤링 완료!\n",
      "800 번째, 20050630 까지 크롤링 완료!\n",
      "810 번째, 20050714 까지 크롤링 완료!\n",
      "820 번째, 20050728 까지 크롤링 완료!\n",
      "830 번째, 20050811 까지 크롤링 완료!\n",
      "840 번째, 20050826 까지 크롤링 완료!\n",
      "850 번째, 20050909 까지 크롤링 완료!\n",
      "860 번째, 20050926 까지 크롤링 완료!\n",
      "870 번째, 20051011 까지 크롤링 완료!\n",
      "880 번째, 20051025 까지 크롤링 완료!\n",
      "890 번째, 20051108 까지 크롤링 완료!\n",
      "900 번째, 20051122 까지 크롤링 완료!\n",
      "910 번째, 20051206 까지 크롤링 완료!\n",
      "920 번째, 20051220 까지 크롤링 완료!\n",
      "930 번째, 20060104 까지 크롤링 완료!\n",
      "940 번째, 20060118 까지 크롤링 완료!\n",
      "950 번째, 20060202 까지 크롤링 완료!\n",
      "960 번째, 20060216 까지 크롤링 완료!\n",
      "970 번째, 20060303 까지 크롤링 완료!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-20e092155125>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m303\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscrapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-128-51b273457ff4>\u001b[0m in \u001b[0;36mscrapping\u001b[1;34m(date)\u001b[0m\n\u001b[0;32m     29\u001b[0m                         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://finance.naver.com'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                         \u001b[0mhtml\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#뉴스링크에 들어가기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                         \u001b[0mbs_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains_replacement_characters\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m              self.builder.prepare_markup(\n\u001b[1;32m--> 225\u001b[1;33m                  markup, from_encoding, exclude_encodings=exclude_encodings)):\n\u001b[0m\u001b[0;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py\u001b[0m in \u001b[0;36mprepare_markup\u001b[1;34m(self, markup, user_specified_encoding, document_declared_encoding, exclude_encodings)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mtry_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0muser_specified_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_declared_encoding\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n\u001b[1;32m--> 205\u001b[1;33m                                exclude_encodings=exclude_encodings)\n\u001b[0m\u001b[0;32m    206\u001b[0m         yield (dammit.markup, dammit.original_encoding,\n\u001b[0;32m    207\u001b[0m                \u001b[0mdammit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeclared_html_encoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\dammit.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, override_encodings, smart_quotes_to, is_html, exclude_encodings)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m             \u001b[0mmarkup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\dammit.py\u001b[0m in \u001b[0;36mencodings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;31m# encoding.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchardet_dammit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_usable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtried\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\dammit.py\u001b[0m in \u001b[0;36mchardet_dammit\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mchardet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mchardet_dammit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mchardet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;31m#import chardet.constants\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m#chardet.constants._debug = 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\chardet\\__init__.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(byte_str)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mbyte_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUniversalDetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\chardet\\universaldetector.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLatin1Prober\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mprober\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mProbingState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFOUND_IT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m                     self.result = {'encoding': prober.charset_name,\n\u001b[0;32m    213\u001b[0m                                    \u001b[1;34m'confidence'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_confidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\chardet\\charsetgroupprober.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\chardet\\mbcharsetprober.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                     self.distribution_analyzer.feed(byte_str[i - 1:i + 1],\n\u001b[1;32m---> 79\u001b[1;33m                                                     char_len)\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbyte_str\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "date = date_update()\n",
    "date = date[303:]\n",
    "scrapping(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 너무 오래 걸림.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
